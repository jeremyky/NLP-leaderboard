[
  {
    "name": "AG News (HF, larger sample)",
    "hf_dataset": "ag_news",
    "config": "default",
    "split": "test",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "cohens_kappa",
      "micro_f1"
    ],
    "suggested_num_samples": 1000,
    "reason": "Classic 4-way news topic classification benchmark; great for general-domain text classification."
  },
  {
    "name": "SST-2 (HF, full)",
    "hf_dataset": "glue",
    "config": "sst2",
    "split": "validation",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "cohens_kappa",
      "micro_f1"
    ],
    "suggested_num_samples": 872,
    "reason": "Binary sentiment analysis on movie reviews; widely used to measure sentiment capability."
  },
  {
    "name": "MNLI (NLI, general-domain)",
    "hf_dataset": "glue",
    "config": "mnli",
    "split": "validation_matched",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "cohens_kappa",
      "micro_f1"
    ],
    "suggested_num_samples": 2000,
    "reason": "Multi-genre natural language inference (entailment/contradiction/neutral) for reasoning over paired sentences."
  },
  {
    "name": "QQP (Paraphrase identification)",
    "hf_dataset": "glue",
    "config": "qqp",
    "split": "validation",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "cohens_kappa",
      "micro_f1"
    ],
    "suggested_num_samples": 2000,
    "reason": "Paraphrase detection on Quora questions; checks semantic similarity and duplicate-question detection."
  },
  {
    "name": "SQuAD v1.1 (HF, larger slice)",
    "hf_dataset": "squad",
    "config": "plain_text",
    "split": "validation",
    "task_type": "document_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1000,
    "reason": "Span-based QA over Wikipedia passages; standard benchmark for extractive question answering."
  },
  {
    "name": "Natural Questions (short answer subset)",
    "hf_dataset": "natural_questions",
    "config": "default",
    "split": "validation",
    "task_type": "document_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1000,
    "reason": "Real user questions with long documents; tests robustness to noisy contexts and long passages."
  },
  {
    "name": "Financial PhraseBank (sentiment)",
    "hf_dataset": "financial_phrasebank",
    "config": "sentences_allagree",
    "split": "train",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "cohens_kappa",
      "micro_f1"
    ],
    "suggested_num_samples": 800,
    "reason": "Canonical financial sentiment dataset with analyst-written sentences about companies and markets."
  },
  {
    "name": "FiQA Sentiment and Opinion (task A)",
    "hf_dataset": "fiqa",
    "config": "sentiment",
    "split": "train",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "micro_f1"
    ],
    "suggested_num_samples": 500,
    "reason": "Financial microblog and news headlines with fine-grained sentiment; useful for nuanced finance tone modeling."
  },
  {
    "name": "FiQA QA (financial question answering)",
    "hf_dataset": "fiqa",
    "config": "qa",
    "split": "train",
    "task_type": "document_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 500,
    "reason": "Question answering in the financial domain; includes investor and analyst questions with detailed answers."
  },
  {
    "name": "TweetEval Financial Sentiment",
    "hf_dataset": "tweet_eval",
    "config": "sentiment",
    "split": "test",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "micro_f1"
    ],
    "suggested_num_samples": 1000,
    "reason": "Twitter sentiment benchmark including finance-related tweets; good for noisy, short-text sentiment."
  },
  {
    "name": "WNUT17 Emerging Entities in Finance News (NER-like)",
    "hf_dataset": "wnut_17",
    "config": "wnut_17",
    "split": "train",
    "task_type": "named_entity_recognition",
    "primary_metric": "f1",
    "additional_metrics": [
      "precision",
      "recall",
      "partial_f1",
      "partial_precision",
      "partial_recall"
    ],
    "suggested_num_samples": 1000,
    "reason": "Noisy user-generated text with emerging entities, including orgs and products relevant to financial streams."
  },
  {
    "name": "AI2 ARC (easy) Science QA",
    "hf_dataset": "allenai/ai2_arc",
    "config": "ARC-Easy",
    "split": "test",
    "task_type": "line_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 778,
    "reason": "Multiple-choice grade-school science QA; evaluates factual science knowledge."
  },
  {
    "name": "AI2 ARC (challenge) Science QA",
    "hf_dataset": "allenai/ai2_arc",
    "config": "ARC-Challenge",
    "split": "test",
    "task_type": "line_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1172,
    "reason": "Harder science questions requiring reasoning and multi-step inference."
  },
  {
    "name": "MMLU (STEM subset)",
    "hf_dataset": "cais/mmlu",
    "config": "stem",
    "split": "test",
    "task_type": "line_qa",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy"
    ],
    "suggested_num_samples": 500,
    "reason": "High-level STEM multiple-choice questions; robust measure of advanced scientific knowledge."
  },
  {
    "name": "GSM8K (math word problems)",
    "hf_dataset": "gsm8k",
    "config": "main",
    "split": "test",
    "task_type": "line_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1319,
    "reason": "Grade-school math word problems; standard benchmark for step-by-step numerical reasoning."
  },
  {
    "name": "StrategyQA (open-domain reasoning)",
    "hf_dataset": "strategyqa",
    "config": "default",
    "split": "validation",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy"
    ],
    "suggested_num_samples": 500,
    "reason": "Yes/no questions requiring multi-hop reasoning; good for testing implicit knowledge and reasoning."
  },
  {
    "name": "HumanEval (code generation QA-style)",
    "hf_dataset": "openai_humaneval",
    "config": "default",
    "split": "test",
    "task_type": "line_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 164,
    "reason": "Functional code generation tasks; good for evaluating code synthesis and reasoning."
  },
  {
    "name": "MBPP (Mostly Basic Python Problems)",
    "hf_dataset": "mbpp",
    "config": "sanitized",
    "split": "test",
    "task_type": "line_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 500,
    "reason": "Python coding problems from simple to moderate difficulty; covers everyday coding ability."
  },
  {
    "name": "CodeXGLUE Text-to-Code (small slice)",
    "hf_dataset": "code_x_glue_tc_text_to_code",
    "config": "python",
    "split": "test",
    "task_type": "line_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 500,
    "reason": "Natural language to Python function generation; good for docstring-to-code capabilities."
  },
  {
    "name": "TruthfulQA (HF subset)",
    "hf_dataset": "truthful_qa",
    "config": "multiple_choice",
    "split": "validation",
    "task_type": "line_qa",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall"
    ],
    "suggested_num_samples": 400,
    "reason": "Measures how often models give truthful vs common-but-false answers."
  },
  {
    "name": "RealToxicityPrompts (generation prompts, classified)",
    "hf_dataset": "allenai/real-toxicity-prompts",
    "config": "default",
    "split": "train",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy"
    ],
    "suggested_num_samples": 1000,
    "reason": "Prompts annotated with toxicity scores; useful for measuring toxic generation risk."
  },
  {
    "name": "Civil Comments (toxicity classification)",
    "hf_dataset": "civil_comments",
    "config": "default",
    "split": "test",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "micro_f1"
    ],
    "suggested_num_samples": 1000,
    "reason": "Long-form comments with toxicity and identity-attack labels; good for safety and fairness evaluation."
  },
  {
    "name": "CNN/DailyMail Summarization",
    "hf_dataset": "cnn_dailymail",
    "config": "3.0.0",
    "split": "test",
    "task_type": "document_qa",
    "primary_metric": "f1",
    "additional_metrics": [
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1000,
    "reason": "News summarization; we can frame summaries as answers to 'summarize' questions on long contexts."
  },
  {
    "name": "XSum Extreme Summarization",
    "hf_dataset": "xsum",
    "config": "default",
    "split": "test",
    "task_type": "document_qa",
    "primary_metric": "f1",
    "additional_metrics": [
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1000,
    "reason": "Single-sentence extreme summarization; evaluates concise abstractive summarization."
  },
  {
    "name": "PubMedQA (long scientific abstract summarization/QA)",
    "hf_dataset": "pubmed_qa",
    "config": "pqa_labeled",
    "split": "train",
    "task_type": "document_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 500,
    "reason": "Biomedical question answering over scientific abstracts; can also be viewed as targeted summarization."
  },
  {
    "name": "MultiWOZ 2.2 (task-oriented dialogue)",
    "hf_dataset": "multi_woz_v22",
    "config": "dialogues",
    "split": "validation",
    "task_type": "line_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "bleu"
    ],
    "suggested_num_samples": 1000,
    "reason": "Multi-domain task-oriented dialogues; good for assessing instruction following in dialogue context."
  },
  {
    "name": "DSTC7 Ubuntu Dialogue Corpus (technical dialogue)",
    "hf_dataset": "ubuntu_dialogs_corpus",
    "config": "default",
    "split": "test",
    "task_type": "retrieval",
    "primary_metric": "retrieval_accuracy",
    "additional_metrics": [
      "mrr",
      "precision_at_1",
      "precision_at_3",
      "recall_at_3"
    ],
    "suggested_num_samples": 1000,
    "reason": "Technical support dialogue retrieval; tests ability to pick the right next utterance."
  },
  {
    "name": "Anthropic Helpful-Harmless (small text_class slice)",
    "hf_dataset": "Anthropic/hh-rlhf",
    "config": "default",
    "split": "train",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy"
    ],
    "suggested_num_samples": 1000,
    "reason": "Pairs of helpful vs harmful responses; we can turn into a preference/label classification task."
  },
  {
    "name": "PAWS-X (cross-lingual paraphrase)",
    "hf_dataset": "paws-x",
    "config": "en",
    "split": "test",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "micro_f1"
    ],
    "suggested_num_samples": 2000,
    "reason": "Cross-lingual paraphrase detection; multiple languages plus English to measure semantic equivalence."
  },
  {
    "name": "XNLI (full multilingual NLI)",
    "hf_dataset": "xnli",
    "config": "all_languages",
    "split": "validation",
    "task_type": "text_classification",
    "primary_metric": "accuracy",
    "additional_metrics": [
      "f1",
      "precision",
      "recall",
      "balanced_accuracy",
      "micro_f1"
    ],
    "suggested_num_samples": 5000,
    "reason": "Multilingual NLI in 15 languages; excellent for cross-lingual reasoning and transfer."
  },
  {
    "name": "TyDi QA (typologically diverse QA)",
    "hf_dataset": "tydiqa",
    "config": "secondary_task",
    "split": "validation",
    "task_type": "document_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1000,
    "reason": "Question answering across many typologically diverse languages with varying scripts and structures."
  },
  {
    "name": "Tatoeba Similarity (multilingual retrieval)",
    "hf_dataset": "tatoeba",
    "config": "default",
    "split": "test",
    "task_type": "retrieval",
    "primary_metric": "retrieval_accuracy",
    "additional_metrics": [
      "mrr",
      "precision_at_1",
      "precision_at_3",
      "recall_at_3"
    ],
    "suggested_num_samples": 2000,
    "reason": "Sentence-level similarity across many language pairs; good for multilingual retrieval and alignment."
  },
  {
    "name": "MLQA (multilingual QA)",
    "hf_dataset": "mlqa",
    "config": "mlqa-translate-train-all-lang",
    "split": "validation",
    "task_type": "document_qa",
    "primary_metric": "exact_match",
    "additional_metrics": [
      "f1",
      "token_f1",
      "bleu",
      "answer_length_ratio"
    ],
    "suggested_num_samples": 1000,
    "reason": "Parallel QA dataset across multiple languages; complements XQUAD and XQUAD-style data."
  }
]