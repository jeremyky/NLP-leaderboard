name: Critical Bug Prevention

# Run on every push to catch regressions of critical bugs
on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

jobs:
  check-duplicate-datasets-bug:
    name: Check for Duplicate Dataset Bug
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Initialize and seed database
      run: |
        python init_db.py
    
    - name: Test for duplicate dataset bug
      run: |
        python -c "
from database import SessionLocal, init_db
from models import Dataset, Submission, SubmissionStatus
from collections import Counter

init_db()
db = SessionLocal()

# Create a test dataset with multiple submissions
import uuid
from models import TaskType

dataset_id = str(uuid.uuid4())
from models import Dataset as DatasetModel
dataset = DatasetModel(
    id=dataset_id,
    name='Bug Check Dataset',
    description='Test',
    url='https://example.com',
    task_type=TaskType.TEXT_CLASSIFICATION,
    test_set_public=False,
    labels_public=False,
    primary_metric='accuracy',
    additional_metrics=[],
    num_examples=2,
    ground_truth=[
        {'id': '1', 'question': 'Q1', 'answer': 'A'},
        {'id': '2', 'question': 'Q2', 'answer': 'B'},
    ]
)
db.add(dataset)
db.flush()

# Add 5 submissions
for i in range(5):
    from models import Submission as SubmissionModel
    sub = SubmissionModel(
        id=str(uuid.uuid4()),
        dataset_id=dataset_id,
        model_name=f'Model {i+1}',
        predictions=[
            {'id': '1', 'prediction': 'A'},
            {'id': '2', 'prediction': 'B'},
        ],
        status=SubmissionStatus.COMPLETED,
        primary_score=0.95 - (i * 0.05),
        detailed_scores={'accuracy': 0.95 - (i * 0.05)},
        is_internal=True
    )
    db.add(sub)

db.commit()
db.close()

# Now test the API
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)
response = client.get('/api/leaderboard')
assert response.status_code == 200

data = response.json()

# Check for duplicates
dataset_counter = Counter(lb['dataset_id'] for lb in data)
duplicates = [(ds_id, count) for ds_id, count in dataset_counter.items() if count > 1]

if duplicates:
    print('❌ CRITICAL BUG DETECTED: Duplicate datasets in leaderboard!')
    for ds_id, count in duplicates:
        ds_name = next(lb['dataset_name'] for lb in data if lb['dataset_id'] == ds_id)
        print(f'   {ds_name}: appears {count} times')
    exit(1)

# Check that Bug Check Dataset has all 5 submissions in ONE entry
bug_check_entries = [lb for lb in data if lb['dataset_name'] == 'Bug Check Dataset']
assert len(bug_check_entries) == 1, f'Bug Check Dataset appears {len(bug_check_entries)} times (should be 1)'
assert len(bug_check_entries[0]['entries']) == 5, f'Bug Check Dataset has {len(bug_check_entries[0]['entries'])} entries (should have 5)'

print('✅ No duplicate dataset bug detected')
print(f'✅ All {len(data)} leaderboards have unique dataset_ids')
"

  check-list-type-errors:
    name: Check for List/String Type Errors in Evaluators
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Test evaluators with list answers
      run: |
        python -c "
from evaluators import TextClassificationEvaluator, QAEvaluator

# Test text classification with list answer
tc_eval = TextClassificationEvaluator()
gt_list = [
    {'id': '1', 'question': 'Q', 'answer': ['option_a', 'option_b']},
    {'id': '2', 'question': 'Q', 'answer': 'option_c'},
]
pred_list = [
    {'id': '1', 'prediction': 'option_a'},
    {'id': '2', 'prediction': 'option_c'},
]

try:
    scores = tc_eval.evaluate(gt_list, pred_list)
    print(f'✅ TextClassification handles list answers: {scores}')
except Exception as e:
    print(f'❌ TextClassification failed with list answers: {e}')
    exit(1)

# Test QA with list answer
qa_eval = QAEvaluator()
gt_qa = [
    {'id': '1', 'question': 'Q', 'answer': ['ans1', 'ans2']},
    {'id': '2', 'question': 'Q', 'answer': 'ans3'},
]
pred_qa = [
    {'id': '1', 'prediction': 'ans1'},
    {'id': '2', 'prediction': 'ans3'},
]

try:
    scores = qa_eval.evaluate(gt_qa, pred_qa)
    print(f'✅ QA evaluator handles list answers: {scores}')
except Exception as e:
    print(f'❌ QA evaluator failed with list answers: {e}')
    exit(1)

print('✅ All evaluators handle list/string types correctly')
"

  check-seeding-doesnt-crash:
    name: Verify All Seeding Scripts Complete
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Test init_db completes without errors
      run: |
        python init_db.py
        echo "✅ init_db.py completed successfully"
    
    - name: Verify all dataset types seeded
      run: |
        python -c "
from database import SessionLocal
from models import Dataset, Submission, TaskType

db = SessionLocal()

# Check we have datasets for each task type
datasets = db.query(Dataset).all()
task_types = {d.task_type for d in datasets}

print(f'Task types seeded: {[t.value for t in task_types]}')

# Should have at least text_classification and document_qa
assert TaskType.TEXT_CLASSIFICATION in task_types
assert TaskType.DOCUMENT_QA in task_types

# Check submissions were created
total_submissions = db.query(Submission).count()
print(f'Total submissions: {total_submissions}')
assert total_submissions >= 50, f'Expected at least 50 submissions, got {total_submissions}'

# Check AG News specifically has many models
ag_news = db.query(Dataset).filter(Dataset.name.like('%AG News%')).first()
if ag_news:
    ag_submissions = db.query(Submission).filter(
        Submission.dataset_id == ag_news.id,
        Submission.status == SubmissionStatus.COMPLETED
    ).count()
    print(f'AG News submissions: {ag_submissions}')
    assert ag_submissions >= 10, f'AG News should have at least 10 models, got {ag_submissions}'

db.close()
print('✅ Database seeding verification passed')
"

  check-no-hardcoded-scores:
    name: Verify Scores Are Evaluated, Not Hardcoded
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Verify evaluators are used in seeding
      run: |
        python -c "
# Check that seed scripts use evaluators, not hardcoded scores
import inspect
from seed_data import seed_database
from finance_datasets import seed_finance_datasets
from multilingual_datasets import seed_multilingual_datasets

# Get source code
seed_source = inspect.getsource(seed_database)
finance_source = inspect.getsource(seed_finance_datasets)
multi_source = inspect.getsource(seed_multilingual_datasets)

# Verify evaluator is called
assert 'evaluator.evaluate' in seed_source, 'seed_database not using evaluator'
assert 'evaluator.evaluate' in finance_source, 'finance seeding not using evaluator'
assert 'evaluator.evaluate' in multi_source, 'multilingual seeding not using evaluator'

print('✅ All seeding scripts use evaluators (not hardcoded scores)')
"

  summary:
    name: Critical Checks Summary
    runs-on: ubuntu-latest
    needs:
      - check-duplicate-datasets-bug
      - check-list-type-errors
      - check-seeding-doesnt-crash
      - check-no-hardcoded-scores
    if: always()
    
    steps:
    - name: Report results
      run: |
        echo "Critical bug check results:"
        echo "  Duplicate datasets: ${{ needs.check-duplicate-datasets-bug.result }}"
        echo "  List type handling: ${{ needs.check-list-type-errors.result }}"
        echo "  Seeding completion: ${{ needs.check-seeding-doesnt-crash.result }}"
        echo "  Evaluator usage: ${{ needs.check-no-hardcoded-scores.result }}"
        
        if [ "${{ needs.check-duplicate-datasets-bug.result }}" != "success" ]; then
          echo "❌ CRITICAL: Duplicate dataset bug detected!"
          exit 1
        fi
        
        if [ "${{ needs.check-list-type-errors.result }}" != "success" ]; then
          echo "❌ CRITICAL: List type handling broken!"
          exit 1
        fi
        
        echo "✅ All critical bug checks passed"

